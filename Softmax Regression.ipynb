{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1321f7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92d787c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader_workers(): #@save\n",
    "    \"\"\"Use 4 processes to read the data.\"\"\"\n",
    "    return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48c4f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size, resize=None): #@save\n",
    "    \"\"\"Download the Fashion-MNIST dataset and then load it into memory.\"\"\"\n",
    "    trans = [transforms.ToTensor()]\n",
    "    if resize:\n",
    "        trans.insert(0, transforms.Resize(resize))\n",
    "    trans = transforms.Compose(trans)\n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=\"../data\",\n",
    "                                                    train=True,\n",
    "                                                    transform=trans,\n",
    "                                                    download=True)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=\"../data\",\n",
    "                                                    train=False,\n",
    "                                                    transform=trans,\n",
    "                                                    download=True)\n",
    "    return (data.DataLoader(mnist_train, batch_size, shuffle=True,\n",
    "            num_workers=get_dataloader_workers()),\n",
    "            data.DataLoader(mnist_test, batch_size, shuffle=False,\n",
    "            num_workers=get_dataloader_workers()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fbfbdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "163ce641",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Each example in the raw dataset is a 28×28 image. In this section, we will flatten each image,\n",
    "treating them as vectors of length 784.\n",
    "Because our\n",
    "dataset has 10 classes, our network will have an output dimension of 10. Consequently, our weights\n",
    "will constitute a 784 × 10 matrix and the biases will constitute a 1 × 10 row vector. \n",
    "'''\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "'''\n",
    "Returns a tensor of random numbers drawn from separate normal distributions \n",
    "whose mean and standard deviation are given.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6b0e69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential(nn.Flatten(),nn.Linear(num_inputs,num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d3be494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.5000])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.tensor([0, 2])\n",
    "y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "y_hat[[0, 1], y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "36c0c080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Flatten(start_dim=1, end_dim=-1)\n",
       "  (1): Linear(in_features=784, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(layer_type):\n",
    "    if layer_type==nn.Linear:\n",
    "        nn.init.normal_(layer_type.weight,std=0.01)\n",
    "        \n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f67393bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.SGD(net.parameters(),lr=0.1)\n",
    "loss=nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b170cec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmodel.train() tells your model that you are training the model. So effectively layers like dropout, batchnorm etc. which behave different on the train and test procedures know what is going on and hence can behave accordingly.\\nMore details: It sets the mode to train (see source code). You can call either model.eval() or model.train(mode=False) to tell that you are testing. It is somewhat intuitive to expect train function to train model but it does not do that.\\nIt just sets the mode.\\n\\nloss.backward() computes dloss/dx for every parameter x which has requires_grad=True '"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "model.train() tells your model that you are training the model. So effectively layers like dropout, batchnorm etc. which behave different on the train and test procedures know what is going on and hence can behave accordingly.\n",
    "More details: It sets the mode to train (see source code). You can call either model.eval() or model.train(mode=False) to tell that you are testing. It is somewhat intuitive to expect train function to train model but it does not do that.\n",
    "It just sets the mode.\n",
    "\n",
    "loss.backward() computes dloss/dx for every parameter x which has requires_grad=True '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "da0b05ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat,y):\n",
    "    if y_hat.shape[0]>0 and y_hat.shape==y.shape:\n",
    "        pass   \n",
    "    y_hat=y_hat.argmax(1)\n",
    "    correct_pred= y_hat.type(y.dtype)==y\n",
    "    \n",
    "    return float(correct_pred.type(y.dtype).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e3e09d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,train_iter,loss,optim):\n",
    "    \n",
    "    if isinstance(net,torch.nn.Module):\n",
    "        net.train()\n",
    "#     acc=Accumulator()\n",
    "    data_count,total_loss,acc=0,0,0\n",
    "    for X,y in train_iter:\n",
    "        data_count+=y.numel()\n",
    "        y_hat=net(X)\n",
    "        acc+=accuracy(y_hat,y)\n",
    "        l=loss(y_hat,y)\n",
    "        total_loss=+l\n",
    "        if isinstance(optim,torch.optim.Optimizer):\n",
    "            optim.zero_grad()\n",
    "            l.backward()\n",
    "            optim.step()\n",
    "            \n",
    "           \n",
    "        \n",
    "    return total_loss,data_count,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9fb6988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPoch loss- tensor(8.5561e-06, grad_fn=<DivBackward0>) 0.85095\n",
      "EPoch loss- tensor(5.6701e-06, grad_fn=<DivBackward0>) 0.8519833333333333\n",
      "EPoch loss- tensor(7.0816e-06, grad_fn=<DivBackward0>) 0.8528\n",
      "EPoch loss- tensor(6.7737e-06, grad_fn=<DivBackward0>) 0.8533833333333334\n",
      "EPoch loss- tensor(7.2044e-06, grad_fn=<DivBackward0>) 0.8540666666666666\n",
      "EPoch loss- tensor(5.6699e-06, grad_fn=<DivBackward0>) 0.8541833333333333\n",
      "EPoch loss- tensor(6.8782e-06, grad_fn=<DivBackward0>) 0.85565\n",
      "EPoch loss- tensor(4.7540e-06, grad_fn=<DivBackward0>) 0.85695\n",
      "EPoch loss- tensor(7.3248e-06, grad_fn=<DivBackward0>) 0.8569666666666667\n",
      "EPoch loss- tensor(8.4440e-06, grad_fn=<DivBackward0>) 0.8571333333333333\n"
     ]
    }
   ],
   "source": [
    "num_epochs=10\n",
    "for i in range(num_epochs):\n",
    "    total_loss,data_count,acc=train(net,train_iter,loss,optimizer)\n",
    "#     print(net[1].weight)\n",
    "    print('EPoch loss-',total_loss/data_count,acc/data_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7280f23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1482, grad_fn=<NllLossBackward>)\n",
      "tensor(2.1482, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "train(net,train_iter,loss,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45cc4f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Given\n",
    "a matrix X we can sum over all elements (by default) or only over elements in the same axis, i.e.,\n",
    "the same column (axis 0) or the same row (axis 1).\n",
    "'''\n",
    "\n",
    "X=torch.tensor([[1.0,2.0,3.0],[4.0,5.0,6.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6450af37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(X.sum(0,keepdim=True).shape) #torch.Size([1, 3])\n",
    "\n",
    "print(X.sum(0,keepdim=False).shape) #torch.Size([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05779234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4651, 0.0576, 0.0860, 0.1832, 0.2081],\n",
       "         [0.1316, 0.1910, 0.4990, 0.0988, 0.0798]]),\n",
       " tensor([1.0000, 1.0000]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We are now ready to implement the softmax operation. Recall that softmax consists of three steps:\n",
    "(i) we exponentiate each term (using exp); \n",
    "(ii) we sum over each row (we have one row per example\n",
    "in the batch) to get the normalization constant for each example; \n",
    "(iii) we divide each row by its\n",
    "normalization constant, ensuring that the result sums to 1. Before looking at the code, let us recall\n",
    "how this looks expressed as an equation:\n",
    "\n",
    "'''\n",
    "\n",
    "def softmax(X):\n",
    "    X_exp=torch.exp(X)\n",
    "    normalization=X_exp.sum(1,keepdim=True)\n",
    "    return X_exp/normalization\n",
    "\n",
    "X = torch.normal(0, 1, (2, 5))\n",
    "X_prob = softmax(X)\n",
    "X_prob, X_prob.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e974765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return softmax(torch.matmul(X.T,W)+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01c6d7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1405, -1.3529])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b922d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.6700)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e793a254",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
